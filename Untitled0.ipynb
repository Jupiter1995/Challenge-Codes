{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "1OjXKmEGTJFVrYqnVwkTqSGTdR9EThxX_",
      "authorship_tag": "ABX9TyOHH3aShhwCgs/UOvV4ZnY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jupiter1995/Challenge-Codes/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFjWTBjwIyaZ"
      },
      "source": [
        "!wget https://github.com/facebookresearch/fastText/archive/0.2.0.zip\r\n",
        "!unzip 0.2.0.zip\r\n",
        "%cd fastText-0.2.0\r\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjCDFqdUKAj9",
        "outputId": "6bf34d9c-08a5-49af-ddd5-81b885b259d2"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=87d3bcdf6885e221740499501ff948bc6e8225bdd5a5a12136d23622732be756\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngU9vVvQFw5G"
      },
      "source": [
        "import numpy as np\r\n",
        "np.random.seed(42)\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "from gensim.models.fasttext import FastText\r\n",
        "\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\r\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\r\n",
        "from keras.preprocessing import text, sequence\r\n",
        "from keras.callbacks import Callback\r\n",
        "from keras.initializers import Constant\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "import os\r\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv6NQeNuX8e8"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Techlent/NLP/train_job.csv')\r\n",
        "test = pd.read_csv('/content/drive/MyDrive/Techlent/NLP/test_job.csv')\r\n",
        "# submission = pd.read_csv('/content/drive/MyDrive/Techlent/NLP/DataTemp/sample_submission.csv')\r\n",
        "\r\n",
        "X_train = train[\"description\"].values\r\n",
        "y_train = train[\"lable\"].values\r\n",
        "X_test = test[\"description\"].values\r\n",
        "\r\n",
        "\r\n",
        "max_features = 12309\r\n",
        "maxlen = 100\r\n",
        "embed_size = 300\r\n",
        "\r\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\r\n",
        "tokenizer.fit_on_texts(list(X_train) + list(X_test) + list(y_train))\r\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\r\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\r\n",
        "y_train = tokenizer.texts_to_sequences(y_train)\r\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\r\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\r\n",
        "# yy_train = sequence.pad_sequences(y_train, maxlen=1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CO-a_ZEZbre"
      },
      "source": [
        "EMBEDDING_FILE = '/content/drive/MyDrive/Techlent/NLP/crawl-300d-2M.vec'\r\n",
        "\r\n",
        "\r\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\r\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYYLgQCKZEO6"
      },
      "source": [
        "word_index = tokenizer.word_index\r\n",
        "nb_words = min(max_features, len(word_index))\r\n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\r\n",
        "for word, i in word_index.items():\r\n",
        "    if i >= max_features: continue\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None: embedding_matrix[i-1] = embedding_vector"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H8G9XxtKgso"
      },
      "source": [
        "class RocAucEvaluation(Callback):\r\n",
        "    def __init__(self, validation_data=(), interval=1):\r\n",
        "        super(Callback, self).__init__()\r\n",
        "\r\n",
        "        self.interval = interval\r\n",
        "        self.X_val, self.y_val = validation_data\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs={}):\r\n",
        "        if epoch % self.interval == 0:\r\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\r\n",
        "            score = roc_auc_score(self.y_val, y_pred)\r\n",
        "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\r\n",
        "\r\n",
        "\r\n",
        "def get_model():\r\n",
        "    inp = Input(shape=(maxlen, ))\r\n",
        "    x = Embedding(max_features, embed_size, embeddings_initializer=Constant(embedding_matrix))(inp)\r\n",
        "    x = SpatialDropout1D(0.2)(x)\r\n",
        "    x = Bidirectional(GRU(80, return_sequences=True))(x)\r\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\r\n",
        "    max_pool = GlobalMaxPooling1D()(x)\r\n",
        "    conc = concatenate([avg_pool, max_pool])\r\n",
        "    outp = Dense(1, activation=\"softmax\")(conc)\r\n",
        "    \r\n",
        "    model = Model(inputs=inp, outputs=outp)\r\n",
        "    model.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['accuracy'])\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "model = get_model()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "iPHuA_XnU6Ys",
        "outputId": "8b864de6-b229-4f67-d2b1-3bbde5e81433"
      },
      "source": [
        "batch_size = 32\r\n",
        "epochs = 2\r\n",
        "\r\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\r\n",
        "# RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\r\n",
        "\r\n",
        "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\r\n",
        "                 callbacks=[RocAuc], verbose=2)\r\n",
        "\r\n",
        "\r\n",
        "y_pred = model.predict(x_test, batch_size=1024)\r\n",
        "y_pred\r\n",
        "# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\r\n",
        "# submission.to_csv('submission.csv', index=False)\r\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fea3bd19f9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n\u001b[0;32m----> 8\u001b[0;31m                  callbacks=[RocAuc], verbose=2)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJmLzpl8zNOk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7KCAhB8LvHE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}